{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Titanic: Machine Learning from Disaster - Applying Machine Learning Techniques \n",
    "\n",
    "Homepage: https://github.com/tien-le/kaggle-titanic\n",
    "\n",
    "**unbelivable ... to achieve 1.000. How did they do this?**\n",
    "\n",
    "**Just curious, how did they cheat the score?** ANS: maybe, we have the information existing in https://www.encyclopedia-titanica.org/titanic-victims/\n",
    "\n",
    "## Competition Description\n",
    "\n",
    "The sinking of the RMS Titanic is one of the most infamous shipwrecks in history.  On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. This sensational tragedy shocked the international community and led to better safety regulations for ships.\n",
    "\n",
    "One of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew. Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\n",
    "\n",
    "In this challenge, we ask you to complete the analysis of what sorts of people were likely to survive. In particular, we ask you to apply the tools of machine learning to predict which passengers survived the tragedy.\n",
    "\n",
    "## References\n",
    "https://www.kaggle.com/c/titanic\n",
    "\n",
    "https://triangleinequality.wordpress.com/2013/09/08/basic-feature-engineering-with-the-titanic-data/\n",
    "\n",
    "https://triangleinequality.wordpress.com/2013/05/19/machine-learning-with-python-first-steps-munging/\n",
    "\n",
    "https://www.kaggle.com/mrisdal/exploring-survival-on-the-titanic\n",
    "\n",
    "https://github.com/justmarkham/scikit-learn-videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Corpus After Preprocessing ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Training Corpus\n",
    "trn_corpus_after_preprocessing = pd.read_csv(\"output/trn_corpus_after_preprocessing.csv\")\n",
    "\n",
    "#Testing Corpus\n",
    "tst_corpus_after_preprocessing = pd.read_csv(\"output/tst_corpus_after_preprocessing.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#tst_corpus_after_preprocessing[tst_corpus_after_preprocessing[\"Fare\"].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 890 entries, 0 to 889\n",
      "Data columns (total 13 columns):\n",
      "PassengerId          890 non-null int64\n",
      "Male                 890 non-null int64\n",
      "Pclass               890 non-null int64\n",
      "Fare                 890 non-null float64\n",
      "FarePerPerson        890 non-null float64\n",
      "Title                890 non-null int64\n",
      "AgeUsingMeanTitle    890 non-null float64\n",
      "AgeClass             890 non-null float64\n",
      "SexClass             890 non-null int64\n",
      "FamilySize           890 non-null int64\n",
      "AgeSquared           890 non-null float64\n",
      "AgeClassSquared      890 non-null float64\n",
      "Survived             890 non-null int64\n",
      "dtypes: float64(6), int64(7)\n",
      "memory usage: 90.5 KB\n",
      "------------------------------------\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 418 entries, 0 to 417\n",
      "Data columns (total 13 columns):\n",
      "PassengerId          418 non-null int64\n",
      "Male                 418 non-null int64\n",
      "Pclass               418 non-null int64\n",
      "Fare                 418 non-null float64\n",
      "FarePerPerson        418 non-null float64\n",
      "Title                418 non-null int64\n",
      "AgeUsingMeanTitle    418 non-null float64\n",
      "AgeClass             418 non-null float64\n",
      "SexClass             418 non-null int64\n",
      "FamilySize           418 non-null int64\n",
      "AgeSquared           418 non-null float64\n",
      "AgeClassSquared      418 non-null float64\n",
      "Survived             418 non-null int64\n",
      "dtypes: float64(6), int64(7)\n",
      "memory usage: 42.5 KB\n"
     ]
    }
   ],
   "source": [
    "trn_corpus_after_preprocessing.info()\n",
    "print(\"-\"*36)\n",
    "tst_corpus_after_preprocessing.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic & Advanced machine learning tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agenda\n",
    "\n",
    "- What is machine learning?\n",
    "- What are the two main categories of machine learning?\n",
    "- What are some examples of machine learning?\n",
    "- How does machine learning \"work\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is machine learning?\n",
    "\n",
    "One definition: \"Machine learning is the semi-automated extraction of knowledge from data\"\n",
    "\n",
    "- **Knowledge from data**: Starts with a question that might be answerable using data\n",
    "- **Automated extraction**: A computer provides the insight\n",
    "- **Semi-automated**: Requires many smart decisions by a human"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are the two main categories of machine learning?\n",
    "\n",
    "**Supervised learning**: Making predictions using data\n",
    "    \n",
    "- Example: Is a given email \"spam\" or \"ham\"?\n",
    "- There is an outcome we are trying to predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Unsupervised learning**: Extracting structure from data\n",
    "\n",
    "- Example: Segment grocery store shoppers into clusters that exhibit similar behaviors\n",
    "- There is no \"right answer\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How does machine learning \"work\"?\n",
    "\n",
    "High-level steps of supervised learning:\n",
    "\n",
    "1. First, train a **machine learning model** using **labeled data**\n",
    "\n",
    "    - \"Labeled data\" has been labeled with the outcome\n",
    "    - \"Machine learning model\" learns the relationship between the attributes of the data and its outcome\n",
    "\n",
    "2. Then, make **predictions** on **new data** for which the label is unknown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The primary goal of supervised learning is to build a model that \"generalizes\": It accurately predicts the **future** rather than the **past**!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions about machine learning\n",
    "\n",
    "- How do I choose **which attributes** of my data to include in the model?\n",
    "- How do I choose **which model** to use?\n",
    "- How do I **optimize** this model for best performance?\n",
    "- How do I ensure that I'm building a model that will **generalize** to unseen data?\n",
    "- Can I **estimate** how well my model is likely to perform on unseen data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benefits and drawbacks of scikit-learn\n",
    "\n",
    "### Benefits:\n",
    "\n",
    "- **Consistent interface** to machine learning models\n",
    "- Provides many **tuning parameters** but with **sensible defaults**\n",
    "- Exceptional **documentation**\n",
    "- Rich set of functionality for **companion tasks**\n",
    "- **Active community** for development and support\n",
    "\n",
    "### Potential drawbacks:\n",
    "\n",
    "- Harder (than R) to **get started with machine learning**\n",
    "- Less emphasis (than R) on **model interpretability**\n",
    "\n",
    "### Further reading:\n",
    "\n",
    "- Ben Lorica: [Six reasons why I recommend scikit-learn](http://radar.oreilly.com/2013/12/six-reasons-why-i-recommend-scikit-learn.html)\n",
    "- scikit-learn authors: [API design for machine learning software](http://arxiv.org/pdf/1309.0238v1.pdf)\n",
    "- Data School: [Should you teach Python or R for data science?](http://www.dataschool.io/python-or-r-for-data-science/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of supervised learning\n",
    "\n",
    "- **Classification:** Predict a categorical response\n",
    "- **Regression:** Predict a ordered/continuous response\n",
    "\n",
    "- Note that each value we are predicting is the **response** (also known as: target, outcome, label, dependent variable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model evaluation metrics\n",
    "\n",
    "- **Regression problems:** Mean Absolute Error, Mean Squared Error, Root Mean Squared Error\n",
    "- **Classification problems:** Classification accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['PassengerId', 'Male', 'Pclass', 'Fare', 'FarePerPerson', 'Title',\n",
       "       'AgeUsingMeanTitle', 'AgeClass', 'SexClass', 'FamilySize', 'AgeSquared',\n",
       "       'AgeClassSquared', 'Survived'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn_corpus_after_preprocessing.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_of_non_preditor_variables = ['Survived','PassengerId']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Method 1\n",
    "#x_train = trn_corpus_after_preprocessing.ix[:, trn_corpus_after_preprocessing.columns != 'Survived']\n",
    "#y_train = trn_corpus_after_preprocessing.ix[:,\"Survived\"]\n",
    "\n",
    "#Method 2\n",
    "x_train = trn_corpus_after_preprocessing[trn_corpus_after_preprocessing.columns.difference(list_of_non_preditor_variables)].copy()\n",
    "y_train = trn_corpus_after_preprocessing['Survived'].copy()\n",
    "#y_train = trn_corpus_after_preprocessing.iloc[:,-1]\n",
    "#y_train = trn_corpus_after_preprocessing[trn_corpus_after_preprocessing.columns[-1]]\n",
    "\n",
    "#x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['AgeClass', 'AgeClassSquared', 'AgeSquared', 'AgeUsingMeanTitle',\n",
       "       'FamilySize', 'Fare', 'FarePerPerson', 'Male', 'Pclass', 'SexClass',\n",
       "       'Title'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# check the types of the features and response\n",
    "#print(type(x_train))\n",
    "#print(type(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Method 1\n",
    "#x_test = tst_corpus_after_preprocessing.ix[:, trn_corpus_after_preprocessing.columns != 'Survived']\n",
    "#y_test = tst_corpus_after_preprocessing.ix[:,\"Survived\"]\n",
    "\n",
    "#Method 2\n",
    "x_test = tst_corpus_after_preprocessing[tst_corpus_after_preprocessing.columns.difference(list_of_non_preditor_variables)].copy()\n",
    "y_test = tst_corpus_after_preprocessing['Survived'].copy()\n",
    "#y_test = tst_corpus_after_preprocessing.iloc[:,-1]\n",
    "#y_test = tst_corpus_after_preprocessing[tst_corpus_after_preprocessing.columns[-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AgeClass</th>\n",
       "      <th>AgeClassSquared</th>\n",
       "      <th>AgeSquared</th>\n",
       "      <th>AgeUsingMeanTitle</th>\n",
       "      <th>FamilySize</th>\n",
       "      <th>Fare</th>\n",
       "      <th>FarePerPerson</th>\n",
       "      <th>Male</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>SexClass</th>\n",
       "      <th>Title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>66.0</td>\n",
       "      <td>4356.0</td>\n",
       "      <td>484.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>3.62500</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38.0</td>\n",
       "      <td>1444.0</td>\n",
       "      <td>1444.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>35.64165</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>78.0</td>\n",
       "      <td>6084.0</td>\n",
       "      <td>676.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>7.92500</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>35.0</td>\n",
       "      <td>1225.0</td>\n",
       "      <td>1225.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>26.55000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>105.0</td>\n",
       "      <td>11025.0</td>\n",
       "      <td>1225.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>8.05000</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   AgeClass  AgeClassSquared  AgeSquared  AgeUsingMeanTitle  FamilySize  \\\n",
       "0      66.0           4356.0       484.0               22.0           1   \n",
       "1      38.0           1444.0      1444.0               38.0           1   \n",
       "2      78.0           6084.0       676.0               26.0           0   \n",
       "3      35.0           1225.0      1225.0               35.0           1   \n",
       "4     105.0          11025.0      1225.0               35.0           0   \n",
       "\n",
       "      Fare  FarePerPerson  Male  Pclass  SexClass  Title  \n",
       "0   7.2500        3.62500     1       3         3      3  \n",
       "1  71.2833       35.64165     0       1         0      3  \n",
       "2   7.9250        7.92500     0       3         0      3  \n",
       "3  53.1000       26.55000     0       1         0      3  \n",
       "4   8.0500        8.05000     1       3         3      3  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display the first 5 rows\n",
    "x_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AgeClass</th>\n",
       "      <th>AgeClassSquared</th>\n",
       "      <th>AgeSquared</th>\n",
       "      <th>AgeUsingMeanTitle</th>\n",
       "      <th>FamilySize</th>\n",
       "      <th>Fare</th>\n",
       "      <th>FarePerPerson</th>\n",
       "      <th>Male</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>SexClass</th>\n",
       "      <th>Title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>885</th>\n",
       "      <td>117.000000</td>\n",
       "      <td>13689.00000</td>\n",
       "      <td>1521.000000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>29.125</td>\n",
       "      <td>4.854167</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886</th>\n",
       "      <td>54.000000</td>\n",
       "      <td>2916.00000</td>\n",
       "      <td>729.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>13.000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>19.000000</td>\n",
       "      <td>361.00000</td>\n",
       "      <td>361.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>30.000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>86.061263</td>\n",
       "      <td>7406.54097</td>\n",
       "      <td>822.948997</td>\n",
       "      <td>28.687088</td>\n",
       "      <td>3</td>\n",
       "      <td>23.450</td>\n",
       "      <td>5.862500</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>26.000000</td>\n",
       "      <td>676.00000</td>\n",
       "      <td>676.000000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>30.000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       AgeClass  AgeClassSquared   AgeSquared  AgeUsingMeanTitle  FamilySize  \\\n",
       "885  117.000000      13689.00000  1521.000000          39.000000           5   \n",
       "886   54.000000       2916.00000   729.000000          27.000000           0   \n",
       "887   19.000000        361.00000   361.000000          19.000000           0   \n",
       "888   86.061263       7406.54097   822.948997          28.687088           3   \n",
       "889   26.000000        676.00000   676.000000          26.000000           0   \n",
       "\n",
       "       Fare  FarePerPerson  Male  Pclass  SexClass  Title  \n",
       "885  29.125       4.854167     0       3         0      3  \n",
       "886  13.000      13.000000     1       2         2      0  \n",
       "887  30.000      30.000000     0       1         0      3  \n",
       "888  23.450       5.862500     0       3         0      3  \n",
       "889  30.000      30.000000     1       1         1      3  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display the last 5 rows\n",
    "x_train.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(890, 11)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the shape of the DataFrame (rows, columns)\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the features?\n",
    "- **AgeClass:** \n",
    "- **AgeClassSquared:** \n",
    "- **AgeSquared:** \n",
    "- ...\n",
    "\n",
    "What is the response?\n",
    "- **Survived:** 1-Yes, 0-No\n",
    "\n",
    "What else do we know?\n",
    "- Because the response variable is dicrete, this is a **Classification** problem.\n",
    "- There are 200 **observations** (represented by the rows), and each observation is a single market.\n",
    "\n",
    "Note that if the response variable is continuous, this is a **regression** problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(890, 11)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AgeClass</th>\n",
       "      <th>AgeClassSquared</th>\n",
       "      <th>AgeSquared</th>\n",
       "      <th>AgeUsingMeanTitle</th>\n",
       "      <th>FamilySize</th>\n",
       "      <th>Fare</th>\n",
       "      <th>FarePerPerson</th>\n",
       "      <th>Male</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>SexClass</th>\n",
       "      <th>Title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>66.0</td>\n",
       "      <td>4356.0</td>\n",
       "      <td>484.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>3.62500</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38.0</td>\n",
       "      <td>1444.0</td>\n",
       "      <td>1444.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>35.64165</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>78.0</td>\n",
       "      <td>6084.0</td>\n",
       "      <td>676.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>7.92500</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>35.0</td>\n",
       "      <td>1225.0</td>\n",
       "      <td>1225.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>26.55000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>105.0</td>\n",
       "      <td>11025.0</td>\n",
       "      <td>1225.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>8.05000</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   AgeClass  AgeClassSquared  AgeSquared  AgeUsingMeanTitle  FamilySize  \\\n",
       "0      66.0           4356.0       484.0               22.0           1   \n",
       "1      38.0           1444.0      1444.0               38.0           1   \n",
       "2      78.0           6084.0       676.0               26.0           0   \n",
       "3      35.0           1225.0      1225.0               35.0           1   \n",
       "4     105.0          11025.0      1225.0               35.0           0   \n",
       "\n",
       "      Fare  FarePerPerson  Male  Pclass  SexClass  Title  \n",
       "0   7.2500        3.62500     1       3         3      3  \n",
       "1  71.2833       35.64165     0       1         0      3  \n",
       "2   7.9250        7.92500     0       3         0      3  \n",
       "3  53.1000       26.55000     0       1         0      3  \n",
       "4   8.0500        8.05000     1       3         3      3  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AgeClass</th>\n",
       "      <th>AgeClassSquared</th>\n",
       "      <th>AgeSquared</th>\n",
       "      <th>AgeUsingMeanTitle</th>\n",
       "      <th>FamilySize</th>\n",
       "      <th>Fare</th>\n",
       "      <th>FarePerPerson</th>\n",
       "      <th>Male</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>SexClass</th>\n",
       "      <th>Title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>890.000000</td>\n",
       "      <td>890.000000</td>\n",
       "      <td>890.000000</td>\n",
       "      <td>890.000000</td>\n",
       "      <td>890.000000</td>\n",
       "      <td>890.000000</td>\n",
       "      <td>890.000000</td>\n",
       "      <td>890.000000</td>\n",
       "      <td>890.000000</td>\n",
       "      <td>890.000000</td>\n",
       "      <td>890.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>65.093487</td>\n",
       "      <td>5328.589203</td>\n",
       "      <td>1054.854311</td>\n",
       "      <td>29.759723</td>\n",
       "      <td>0.905618</td>\n",
       "      <td>32.231685</td>\n",
       "      <td>19.930045</td>\n",
       "      <td>0.647191</td>\n",
       "      <td>2.307865</td>\n",
       "      <td>1.546067</td>\n",
       "      <td>2.933708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>33.055330</td>\n",
       "      <td>5202.072217</td>\n",
       "      <td>891.137944</td>\n",
       "      <td>13.015510</td>\n",
       "      <td>1.614080</td>\n",
       "      <td>49.714597</td>\n",
       "      <td>35.859086</td>\n",
       "      <td>0.478113</td>\n",
       "      <td>0.836220</td>\n",
       "      <td>1.316424</td>\n",
       "      <td>0.412254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.920000</td>\n",
       "      <td>0.846400</td>\n",
       "      <td>0.176400</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>40.000000</td>\n",
       "      <td>1600.000000</td>\n",
       "      <td>484.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.925000</td>\n",
       "      <td>7.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>63.000000</td>\n",
       "      <td>3969.000000</td>\n",
       "      <td>900.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.454200</td>\n",
       "      <td>8.331250</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>90.000000</td>\n",
       "      <td>8100.000000</td>\n",
       "      <td>1225.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>23.666667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>222.000000</td>\n",
       "      <td>49284.000000</td>\n",
       "      <td>6400.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>512.329200</td>\n",
       "      <td>512.329200</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         AgeClass  AgeClassSquared   AgeSquared  AgeUsingMeanTitle  \\\n",
       "count  890.000000       890.000000   890.000000         890.000000   \n",
       "mean    65.093487      5328.589203  1054.854311          29.759723   \n",
       "std     33.055330      5202.072217   891.137944          13.015510   \n",
       "min      0.920000         0.846400     0.176400           0.420000   \n",
       "25%     40.000000      1600.000000   484.000000          22.000000   \n",
       "50%     63.000000      3969.000000   900.000000          30.000000   \n",
       "75%     90.000000      8100.000000  1225.000000          35.000000   \n",
       "max    222.000000     49284.000000  6400.000000          80.000000   \n",
       "\n",
       "       FamilySize        Fare  FarePerPerson        Male      Pclass  \\\n",
       "count  890.000000  890.000000     890.000000  890.000000  890.000000   \n",
       "mean     0.905618   32.231685      19.930045    0.647191    2.307865   \n",
       "std      1.614080   49.714597      35.859086    0.478113    0.836220   \n",
       "min      0.000000    0.000000       0.000000    0.000000    1.000000   \n",
       "25%      0.000000    7.925000       7.250000    0.000000    2.000000   \n",
       "50%      0.000000   14.454200       8.331250    1.000000    3.000000   \n",
       "75%      1.000000   31.000000      23.666667    1.000000    3.000000   \n",
       "max     10.000000  512.329200     512.329200    1.000000    3.000000   \n",
       "\n",
       "         SexClass       Title  \n",
       "count  890.000000  890.000000  \n",
       "mean     1.546067    2.933708  \n",
       "std      1.316424    0.412254  \n",
       "min      0.000000    0.000000  \n",
       "25%      0.000000    3.000000  \n",
       "50%      2.000000    3.000000  \n",
       "75%      3.000000    3.000000  \n",
       "max      3.000000    3.000000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "display(x_train.head())\n",
    "display(x_train.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Trees Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf = clf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Once trained, we can export the tree in Graphviz format using the export_graphviz exporter. \n",
    "#Below is an example export of a tree trained on the entire iris dataset:\n",
    "with open(\"output/titanic.dot\", 'w') as f:\n",
    "    f = tree.export_graphviz(clf, out_file=f)\n",
    "\n",
    "#Then we can use Graphvizâ€™s dot tool to create a PDF file (or any other supported file type): \n",
    "#dot -Tpdf titanic.dot -o titanic.pdf.\n",
    "import os\n",
    "os.unlink('output/titanic.dot')\n",
    "\n",
    "#Alternatively, if we have Python module pydotplus installed, we can generate a PDF file \n",
    "#(or any other supported file type) directly in Python:\n",
    "import pydotplus \n",
    "dot_data = tree.export_graphviz(clf, out_file=None) \n",
    "graph = pydotplus.graph_from_dot_data(dot_data) \n",
    "graph.write_pdf(\"output/titanic.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from IPython.display import Image  \\ndot_data = tree.export_graphviz(clf, out_file=None, \\n                         feature_names= list(x_train.columns[1:]), #iris.feature_names,  \\n                         class_names= [\"Survived\"], #iris.target_names,  \\n                         filled=True, rounded=True,  \\n                         special_characters=True)  \\ngraph = pydotplus.graph_from_dot_data(dot_data)  \\nImage(graph.create_png())'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The export_graphviz exporter also supports a variety of aesthetic options, \n",
    "#including coloring nodes by their class (or value for regression) \n",
    "#and using explicit variable and class names if desired. \n",
    "#IPython notebooks can also render these plots inline using the Image() function:\n",
    "\n",
    "\n",
    "\"\"\"from IPython.display import Image  \n",
    "dot_data = tree.export_graphviz(clf, out_file=None, \n",
    "                         feature_names= list(x_train.columns[1:]), #iris.feature_names,  \n",
    "                         class_names= [\"Survived\"], #iris.target_names,  \n",
    "                         filled=True, rounded=True,  \n",
    "                         special_characters=True)  \n",
    "graph = pydotplus.graph_from_dot_data(dot_data)  \n",
    "Image(graph.create_png())\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score:  0.77990430622\n"
     ]
    }
   ],
   "source": [
    "print(\"accuracy score: \", clf.score(x_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classification accuracy:** percentage of correct predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#After being fitted, the model can then be used to predict the class of samples:\n",
    "y_pred_class = clf.predict(x_test);\n",
    "\n",
    "#Alternatively, the probability of each class can be predicted, \n",
    "#which is the fraction of training samples of the same class in a leaf:\n",
    "clf.predict_proba(x_test);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.77990430622\n"
     ]
    }
   ],
   "source": [
    "# calculate accuracy\n",
    "from sklearn import metrics\n",
    "\n",
    "print(metrics.accuracy_score(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Null accuracy:** accuracy that could be achieved by always predicting the most frequent class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    266\n",
       "1    152\n",
       "Name: Survived, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# examine the class distribution of the testing set (using a Pandas Series method)\n",
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.36363636363636365"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate the percentage of ones\n",
    "y_test.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.63636363636363635"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate the percentage of zeros\n",
    "1 - y_test.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.63636363636363635"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate null accuracy (for binary classification problems coded as 0/1)\n",
    "max(y_test.mean(), 1 - y_test.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.636364\n",
       "Name: Survived, dtype: float64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate null accuracy (for multi-class classification problems)\n",
    "y_test.value_counts().head(1) / len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the **true** and **predicted** response values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True: [0 1 0 0 1 0 1 0 1 0 0 0 1 0 1 1 0 0 1 1 0 0 1 0 1]\n",
      "Pred: [0 0 1 1 1 0 0 0 1 0 0 0 1 0 1 1 0 1 0 0 0 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "# print the first 25 true and predicted responses\n",
    "from __future__ import print_function\n",
    "print('True:', y_test.values[0:25])\n",
    "print('Pred:', y_pred_class[0:25])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion:** ???\n",
    "\n",
    "- Classification accuracy is the **easiest classification metric to understand**\n",
    "- But, it does not tell you the **underlying distribution** of response values\n",
    "- And, it does not tell you what **\"types\" of errors** your classifier is making"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion matrix\n",
    "\n",
    "Table that describes the performance of a classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[214  52]\n",
      " [ 40 112]]\n"
     ]
    }
   ],
   "source": [
    "# IMPORTANT: first argument is true values, second argument is predicted values\n",
    "print(metrics.confusion_matrix(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Basic terminology**\n",
    "\n",
    "- **True Positives (TP):** we *correctly* predicted that they *do* have diabetes\n",
    "- **True Negatives (TN):** we *correctly* predicted that they *don't* have diabetes\n",
    "- **False Positives (FP):** we *incorrectly* predicted that they *do* have diabetes (a \"Type I error\")\n",
    "- **False Negatives (FN):** we *incorrectly* predicted that they *don't* have diabetes (a \"Type II error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save confusion matrix and slice into four pieces\n",
    "confusion = metrics.confusion_matrix(y_test, y_pred_class)\n",
    "TP = confusion[1, 1]\n",
    "TN = confusion[0, 0]\n",
    "FP = confusion[0, 1]\n",
    "FN = confusion[1, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112 214 52 40\n"
     ]
    }
   ],
   "source": [
    "print(TP, TN, FP, FN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics computed from a confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classification Accuracy:** Overall, how often is the classifier correct?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.77990430622\n",
      "0.77990430622\n"
     ]
    }
   ],
   "source": [
    "print((TP + TN) / float(TP + TN + FP + FN))\n",
    "print(metrics.accuracy_score(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classification Error:** Overall, how often is the classifier incorrect?\n",
    "\n",
    "- Also known as \"Misclassification Rate\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.22009569378\n",
      "0.22009569378\n"
     ]
    }
   ],
   "source": [
    "print((FP + FN) / float(TP + TN + FP + FN))\n",
    "print(1 - metrics.accuracy_score(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Specificity:** When the actual value is negative, how often is the prediction correct?\n",
    "\n",
    "- How \"specific\" (or \"selective\") is the classifier in predicting positive instances?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.804511278195\n"
     ]
    }
   ],
   "source": [
    "print(TN / float(TN + FP))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**False Positive Rate:** When the actual value is negative, how often is the prediction incorrect?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.195488721805\n"
     ]
    }
   ],
   "source": [
    "print(FP / float(TN + FP))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Precision:** When a positive value is predicted, how often is the prediction correct?\n",
    "\n",
    "- How \"precise\" is the classifier when predicting positive instances?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.682926829268\n",
      "0.682926829268\n"
     ]
    }
   ],
   "source": [
    "print(TP / float(TP + FP))\n",
    "print(metrics.precision_score(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Presicion:  0.682926829268\n",
      "Recall:  0.736842105263\n",
      "F1 score:  0.708860759494\n"
     ]
    }
   ],
   "source": [
    "print(\"Presicion: \", metrics.precision_score(y_test, y_pred_class))\n",
    "print(\"Recall: \", metrics.recall_score(y_test, y_pred_class))\n",
    "print(\"F1 score: \", metrics.f1_score(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many other metrics can be computed: F1 score, Matthews correlation coefficient, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion:**\n",
    "\n",
    "- Confusion matrix gives you a **more complete picture** of how your classifier is performing\n",
    "- Also allows you to compute various **classification metrics**, and these metrics can guide your model selection\n",
    "\n",
    "**Which metrics should you focus on?**\n",
    "\n",
    "- Choice of metric depends on your **business objective**\n",
    "- **Spam filter** (positive class is \"spam\"): Optimize for **precision or specificity** because false negatives (spam goes to the inbox) are more acceptable than false positives (non-spam is caught by the spam filter)\n",
    "- **Fraudulent transaction detector** (positive class is \"fraud\"): Optimize for **sensitivity** because false positives (normal transactions that are flagged as possible fraud) are more acceptable than false negatives (fraudulent transactions that are not detected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Support Vector Classification.\n",
    "\n",
    "Similar to SVC with parameter kernel=â€™linearâ€™, but implemented in terms of liblinear rather than libsvm, so it has more flexibility in the choice of penalties and loss functions and should scale better to large numbers of samples.\n",
    "\n",
    "Ref: http://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "model = svm.LinearSVC()\n",
    "\n",
    "# fit a model to the data\n",
    "model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score:  0.361244019139\n"
     ]
    }
   ],
   "source": [
    "acc_score = model.score(x_test, y_test)\n",
    "\n",
    "print(\"Accuracy score: \", acc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred_class = model.predict(x_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 26 240]\n",
      " [ 27 125]]\n"
     ]
    }
   ],
   "source": [
    "confusion_matrix = metrics.confusion_matrix(y_test, y_pred_class)\n",
    "\n",
    "print(confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.49      0.10      0.16       266\n",
      "          1       0.34      0.82      0.48       152\n",
      "\n",
      "avg / total       0.44      0.36      0.28       418\n",
      "\n",
      "[[ 26 240]\n",
      " [ 27 125]]\n"
     ]
    }
   ],
   "source": [
    "# summarize the fit of the model\n",
    "print(metrics.classification_report(y_test, y_pred_class))\n",
    "print(metrics.confusion_matrix(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Classifier comparison\n",
    "\n",
    "http://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html\n",
    "\n",
    "A comparison of a several classifiers in scikit-learn on synthetic datasets. The point of this example is to illustrate the nature of decision boundaries of different classifiers. This should be taken with a grain of salt, as the intuition conveyed by these examples does not necessarily carry over to real datasets.\n",
    "\n",
    "Particularly in high-dimensional spaces, data can more easily be separated linearly and the simplicity of classifiers such as naive Bayes and linear SVMs might lead to better generalization than is achieved by other classifiers.\n",
    "\n",
    "The plots show training points in solid colors and testing points semi-transparent. The lower right shows the classification accuracy on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from matplotlib.colors import ListedColormap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#sns.pairplot(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train_scaled = StandardScaler().fit_transform(x_train)\n",
    "\n",
    "x_test_scaled = StandardScaler().fit_transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.02743953, -0.187067  , -0.64095047, -0.59652571,  0.05850706,\n",
       "       -0.50278454, -0.4549534 ,  0.73833521,  0.82816049,  1.10507752,\n",
       "        0.1608944 ])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_scaled[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train_scaled[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_x_train_scaled = pd.DataFrame(columns=x_train.columns, data=x_train_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AgeClass</th>\n",
       "      <th>AgeClassSquared</th>\n",
       "      <th>AgeSquared</th>\n",
       "      <th>AgeUsingMeanTitle</th>\n",
       "      <th>FamilySize</th>\n",
       "      <th>Fare</th>\n",
       "      <th>FarePerPerson</th>\n",
       "      <th>Male</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>SexClass</th>\n",
       "      <th>Title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.027440</td>\n",
       "      <td>-0.187067</td>\n",
       "      <td>-0.640950</td>\n",
       "      <td>-0.596526</td>\n",
       "      <td>0.058507</td>\n",
       "      <td>-0.502785</td>\n",
       "      <td>-0.454953</td>\n",
       "      <td>0.738335</td>\n",
       "      <td>0.828160</td>\n",
       "      <td>1.105078</td>\n",
       "      <td>0.160894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.820101</td>\n",
       "      <td>-0.747159</td>\n",
       "      <td>0.436930</td>\n",
       "      <td>0.633468</td>\n",
       "      <td>0.058507</td>\n",
       "      <td>0.785958</td>\n",
       "      <td>0.438395</td>\n",
       "      <td>-1.354398</td>\n",
       "      <td>-1.564901</td>\n",
       "      <td>-1.175106</td>\n",
       "      <td>0.160894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.390671</td>\n",
       "      <td>0.145295</td>\n",
       "      <td>-0.425374</td>\n",
       "      <td>-0.289027</td>\n",
       "      <td>-0.561389</td>\n",
       "      <td>-0.489199</td>\n",
       "      <td>-0.334972</td>\n",
       "      <td>-1.354398</td>\n",
       "      <td>0.828160</td>\n",
       "      <td>-1.175106</td>\n",
       "      <td>0.160894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.910909</td>\n",
       "      <td>-0.789281</td>\n",
       "      <td>0.191038</td>\n",
       "      <td>0.402844</td>\n",
       "      <td>0.058507</td>\n",
       "      <td>0.419998</td>\n",
       "      <td>0.184714</td>\n",
       "      <td>-1.354398</td>\n",
       "      <td>-1.564901</td>\n",
       "      <td>-1.175106</td>\n",
       "      <td>0.160894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.207943</td>\n",
       "      <td>1.095643</td>\n",
       "      <td>0.191038</td>\n",
       "      <td>0.402844</td>\n",
       "      <td>-0.561389</td>\n",
       "      <td>-0.486684</td>\n",
       "      <td>-0.331484</td>\n",
       "      <td>0.738335</td>\n",
       "      <td>0.828160</td>\n",
       "      <td>1.105078</td>\n",
       "      <td>0.160894</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   AgeClass  AgeClassSquared  AgeSquared  AgeUsingMeanTitle  FamilySize  \\\n",
       "0  0.027440        -0.187067   -0.640950          -0.596526    0.058507   \n",
       "1 -0.820101        -0.747159    0.436930           0.633468    0.058507   \n",
       "2  0.390671         0.145295   -0.425374          -0.289027   -0.561389   \n",
       "3 -0.910909        -0.789281    0.191038           0.402844    0.058507   \n",
       "4  1.207943         1.095643    0.191038           0.402844   -0.561389   \n",
       "\n",
       "       Fare  FarePerPerson      Male    Pclass  SexClass     Title  \n",
       "0 -0.502785      -0.454953  0.738335  0.828160  1.105078  0.160894  \n",
       "1  0.785958       0.438395 -1.354398 -1.564901 -1.175106  0.160894  \n",
       "2 -0.489199      -0.334972 -1.354398  0.828160 -1.175106  0.160894  \n",
       "3  0.419998       0.184714 -1.354398 -1.564901 -1.175106  0.160894  \n",
       "4 -0.486684      -0.331484  0.738335  0.828160  1.105078  0.160894  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_x_train_scaled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#sns.pairplot(df_x_train_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How To Compare Machine Learning Algorithms in Python with scikit-learn\n",
    "\n",
    "Ref: http://machinelearningmastery.com/compare-machine-learning-algorithms-python-scikit-learn/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choose The Best Machine Learning Model\n",
    "\n",
    "How do you choose the best model for your problem?\n",
    "\n",
    "When you work on a machine learning project, you often end up with multiple good models to choose from. Each model will have different performance characteristics.\n",
    "\n",
    "Using resampling methods like cross validation, you can get an estimate for how accurate each model may be on unseen data. You need to be able to use these estimates to choose one or two best models from the suite of models that you have created.\n",
    "\n",
    "#### Compare Machine Learning Models Carefully\n",
    "\n",
    "When you have a new dataset, it is a good idea to visualize the data using different techniques in order to look at the data from different perspectives.\n",
    "\n",
    "The same idea applies to model selection. You should use a number of different ways of looking at the estimated accuracy of your machine learning algorithms in order to choose the one or two to finalize.\n",
    "\n",
    "A way to do this is to use different visualization methods to show the average accuracy, variance and other properties of the distribution of model accuracies.\n",
    "\n",
    "#### Compare Machine Learning Algorithms Consistently\n",
    "\n",
    "The key to a fair comparison of machine learning algorithms is ensuring that each algorithm is evaluated in the same way on the same data.\n",
    "\n",
    "You can achieve this by forcing each algorithm to be evaluated on a consistent test harness.\n",
    "\n",
    "In the example below 6 different algorithms are compared:\n",
    "\n",
    "    Logistic Regression\n",
    "    Linear Discriminant Analysis\n",
    "    K-Nearest Neighbors\n",
    "    Classification and Regression Trees\n",
    "    Naive Bayes\n",
    "    Support Vector Machines\n",
    "\n",
    "The problem is a standard binary classification dataset from the UCI machine learning repository called the Pima Indians onset of diabetes problem. The problem has two classes and eight numeric input variables of varying scales.\n",
    "\n",
    "The 10-fold cross validation procedure is used to evaluate each algorithm, importantly configured with the same random seed to ensure that the same splits to the training data are performed and that each algorithms is evaluated in precisely the same way.\n",
    "\n",
    "Each algorithm is given a short name, useful for summarizing results afterward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn import model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR: 0.777528 (+-0.025024)\n",
      "LDA: 0.806742 (+-0.030894)\n",
      "KNN: 0.707865 (+-0.046599)\n",
      "CART: 0.777528 (+-0.032096)\n",
      "NB: 0.770787 (+-0.049540)\n",
      "SVM: 0.670787 (+-0.079775)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEVCAYAAAAM3jVmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fac1c12ec50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# prepare configuration for cross validation test harness\n",
    "seed = 7\n",
    "\n",
    "# prepare models\n",
    "models = []\n",
    "\n",
    "models.append(('LR', LogisticRegression()))\n",
    "models.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "models.append(('KNN', KNeighborsClassifier()))\n",
    "models.append(('CART', DecisionTreeClassifier()))\n",
    "models.append(('NB', GaussianNB()))\n",
    "models.append(('SVM', SVC()))\n",
    "\n",
    "# evaluate each model in turn\n",
    "results = []\n",
    "names = []\n",
    "scoring = 'accuracy'\n",
    "for name, model in models:\n",
    "    kfold = model_selection.KFold(n_splits=10, random_state=seed)\n",
    "    cv_results = model_selection.cross_val_score(model, x_train, y_train, cv=kfold, scoring=scoring)\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    msg = \"%s: %f (+-%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "    print(msg)\n",
    "# boxplot algorithm comparison\n",
    "fig = plt.figure()\n",
    "fig.suptitle('Algorithm Comparison')\n",
    "ax = fig.add_subplot(111)\n",
    "plt.boxplot(results)\n",
    "ax.set_xticklabels(names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comment:\n",
    "From above results, it would suggest that both logistic regression, linear discriminate analysis, CART and NB are perhaps worthy of further study on this problem. Note that, we use **x_train** for cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using some hyperparameters ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest Neighbors  - accuracy score:  0.777511961722\n",
      "Linear SVM  - accuracy score:  1.0\n",
      "RBF SVM  - accuracy score:  0.877990430622\n",
      "Decision Tree  - accuracy score:  0.928229665072\n",
      "Random Forest  - accuracy score:  0.746411483254\n",
      "Neural Net  - accuracy score:  0.913875598086\n",
      "AdaBoost  - accuracy score:  0.858851674641\n",
      "Naive Bayes  - accuracy score:  0.827751196172\n",
      "QDA  - accuracy score:  0.777511961722\n"
     ]
    }
   ],
   "source": [
    "names = [\"Nearest Neighbors\", \"Linear SVM\", \"RBF SVM\",\n",
    "         \"Decision Tree\", \"Random Forest\", \"Neural Net\", \"AdaBoost\",\n",
    "         \"Naive Bayes\", \"QDA\", \"Gaussian Process\"]\n",
    "\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(3),\n",
    "    SVC(kernel=\"linear\", C=0.025),\n",
    "    SVC(gamma=2, C=1),\n",
    "    DecisionTreeClassifier(max_depth=5),\n",
    "    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n",
    "    MLPClassifier(alpha=1),\n",
    "    AdaBoostClassifier(),\n",
    "    GaussianNB(),\n",
    "    QuadraticDiscriminantAnalysis()\n",
    "    #, GaussianProcessClassifier(1.0 * RBF(1.0), warm_start=True), # Take too long...\n",
    "    ]\n",
    "\n",
    "# iterate over classifiers\n",
    "for name, model in zip(names, classifiers):\n",
    "    # fit a model to the data\n",
    "    model.fit(x_train_scaled, y_train)\n",
    "    \n",
    "    # make predictions - not used\n",
    "    \n",
    "    # summarize the fit of the model\n",
    "    acc_score = model.score(x_test_scaled, y_test)\n",
    "    print(name, \" - accuracy score: \", acc_score)\n",
    "#end for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest Neighbors: 0.808989 (+-0.030149)\n",
      "Linear SVM: 0.786517 (+-0.034813)\n",
      "RBF SVM: 0.792135 (+-0.027176)\n",
      "Decision Tree: 0.807865 (+-0.027269)\n",
      "Random Forest: 0.805618 (+-0.032194)\n",
      "Neural Net: 0.786517 (+-0.032565)\n",
      "AdaBoost: 0.804494 (+-0.028953)\n",
      "Naive Bayes: 0.771910 (+-0.035549)\n",
      "QDA: 0.791011 (+-0.029812)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA6sAAAGQCAYAAABIwPbCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJEmSOsdgVZIkSZLUOQarkiRJkqTO+f9R3b5pSe++TgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fabe01d6fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "names_classifiers = [\"Nearest Neighbors\", \"Linear SVM\", \"RBF SVM\",\n",
    "         \"Decision Tree\", \"Random Forest\", \"Neural Net\", \"AdaBoost\",\n",
    "         \"Naive Bayes\", \"QDA\", \"Gaussian Process\"]\n",
    "\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(3),\n",
    "    SVC(kernel=\"linear\", C=0.025),\n",
    "    SVC(gamma=2, C=1),\n",
    "    DecisionTreeClassifier(max_depth=5),\n",
    "    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n",
    "    MLPClassifier(alpha=1),\n",
    "    AdaBoostClassifier(),\n",
    "    GaussianNB(),\n",
    "    QuadraticDiscriminantAnalysis()\n",
    "    #, GaussianProcessClassifier(1.0 * RBF(1.0), warm_start=True), # Take too long...\n",
    "    ]\n",
    "\n",
    "# prepare configuration for cross validation test harness\n",
    "seed = 7\n",
    "\n",
    "models = zip(names_classifiers, classifiers)\n",
    "\n",
    "# evaluate each model in turn\n",
    "results = []\n",
    "names = []\n",
    "scoring = 'accuracy'\n",
    "for name, model in models:\n",
    "    kfold = model_selection.KFold(n_splits=10, random_state=seed)\n",
    "    cv_results = model_selection.cross_val_score(model, x_train_scaled, y_train, cv=kfold, scoring=scoring)\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    msg = \"%s: %f (+-%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "    print(msg)\n",
    "# boxplot algorithm comparison\n",
    "fig = plt.figure(figsize=(16, 6))\n",
    "fig.suptitle('Algorithm Comparison')\n",
    "ax = fig.add_subplot(111)\n",
    "plt.boxplot(results)\n",
    "ax.set_xticklabels(names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comment:\n",
    "From above results, it would suggest that both Nearest Neighbors, Decision Tree, Random Forest and AdaBoost are perhaps worthy of further study on this problem. Note that, we use **x_train_scaled** for cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### How to Develop Your First XGBoost Model in Python with scikit-learn\n",
    "\n",
    "Ref: \n",
    "+ http://machinelearningmastery.com/develop-first-xgboost-model-python-scikit-learn/\n",
    "+ http://machinelearningmastery.com/stochastic-gradient-boosting-xgboost-scikit-learn-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 89.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lavie/.local/lib/python3.5/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# fit model no training data\n",
    "model = XGBClassifier()\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "# make predictions for test data\n",
    "y_pred = model.predict(x_test)\n",
    "predictions = [round(value) for value in y_pred]\n",
    "\n",
    "# evaluate predictions\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 89.00%\n"
     ]
    }
   ],
   "source": [
    "# Save Model Using pickle\n",
    "import pickle\n",
    "\n",
    "# fit model no training data\n",
    "model = XGBClassifier()\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "# save the model to disk\n",
    "filename = 'output/XGBClassifier_model-pickle.sav'\n",
    "pickle.dump(model, open(filename, 'wb'))\n",
    " \n",
    "# some time later...\n",
    " \n",
    "# load the model from disk\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "\n",
    "# make predictions for test data\n",
    "y_pred = loaded_model.predict(x_test)\n",
    "predictions = [round(value) for value in y_pred]\n",
    "\n",
    "# evaluate predictions\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 89.00%\n"
     ]
    }
   ],
   "source": [
    "# Save Model Using joblib\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "# fit model no training data\n",
    "model = XGBClassifier()\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "# save the model to disk\n",
    "filename = 'output/XGBClassifier_model-joblib.sav'\n",
    "joblib.dump(model, filename)\n",
    " \n",
    "# some time later...\n",
    " \n",
    "# load the model from disk\n",
    "loaded_model = joblib.load(filename)\n",
    "\n",
    "# make predictions for test data\n",
    "y_pred = loaded_model.predict(x_test)\n",
    "predictions = [round(value) for value in y_pred]\n",
    "\n",
    "# evaluate predictions\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 84.69%\n"
     ]
    }
   ],
   "source": [
    "# Using data after scaling ...\n",
    "\n",
    "# fit model no training data\n",
    "model = XGBClassifier()\n",
    "model.fit(x_train_scaled, y_train)\n",
    "\n",
    "# make predictions for test data\n",
    "y_pred = model.predict(x_test_scaled)\n",
    "predictions = [round(value) for value in y_pred]\n",
    "\n",
    "# evaluate predictions\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.76404494  0.83146067  0.82022472  0.85393258  0.87640449  0.85393258\n",
      "  0.83146067  0.80898876  0.87640449  0.85393258]\n",
      "max:  0.876404494382\n",
      "min:  0.76404494382\n",
      "mean:  0.837078651685\n",
      "Accuracy: 0.84 (+/- 0.06)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "\n",
    "seed = 7\n",
    "kfold = KFold(n_splits=10, random_state=seed)\n",
    "model = XGBClassifier()\n",
    "results = cross_val_score(model, x_train, y_train, cv=kfold)\n",
    "\n",
    "print(results)\n",
    "print(\"max: \", results.max())\n",
    "print(\"min: \", results.min())\n",
    "print(\"mean: \", results.mean())\n",
    "\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (results.mean(), results.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#for train_indices, test_indices in kfold.split(x_train):\n",
    "#    print('Train: %s | test: %s' % (train_indices, test_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Ensemble Machine Learning Algorithms\n",
    "\n",
    "Ref: http://machinelearningmastery.com/ensemble-machine-learning-algorithms-python-scikit-learn/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Combine Model Predictions Into Ensemble Predictions\n",
    "\n",
    "The three most popular methods for combining the predictions from different models are:\n",
    "\n",
    "   + **Bagging:** Building multiple models (typically of the same type) from different subsamples of the training dataset.\n",
    "   + **Boosting:** Building multiple models (typically of the same type) each of which learns to fix the prediction errors of a prior model in the chain.\n",
    "   + **Voting:** Building multiple models (typically of differing types) and simple statistics (like calculating the mean) are used to combine predictions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each ensemble algorithm is demonstrated using 10 fold cross validation, a standard technique used to estimate the performance of any machine learning algorithm on unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we discovered ensemble machine learning algorithms for improving the performance of models on our problems.\n",
    "+ Bagging Ensembles including Bagged Decision Trees, Random Forest and Extra Trees.\n",
    "+ Boosting Ensembles including AdaBoost and Stochastic Gradient Boosting.\n",
    "+ Voting Ensembles for averaging the predictions for any arbitrary models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging Algorithms\n",
    "\n",
    "Bootstrap Aggregation or bagging involves taking multiple samples from your training dataset (with replacement) and training a model for each sample.\n",
    "\n",
    "The final output prediction is averaged across the predictions of all of the sub-models.\n",
    "\n",
    "The three bagging models covered in this section are as follows:\n",
    "\n",
    "   + Bagged Decision Trees\n",
    "   + Random Forest\n",
    "   + Extra Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagged Decision Trees\n",
    "\n",
    "Bagging performs best with algorithms that have high variance. A popular example are decision trees, often constructed without pruning.\n",
    "\n",
    "In the example below see an example of using the BaggingClassifier with the Classification and Regression Trees algorithm (DecisionTreeClassifier). A total of 100 trees are created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.74157303  0.80898876  0.7752809   0.79775281  0.87640449  0.84269663\n",
      "  0.80898876  0.82022472  0.83146067  0.85393258]\n",
      "max:  0.876404494382\n",
      "min:  0.741573033708\n",
      "mean:  0.815730337079\n",
      "Accuracy: 0.82 (+/- 0.07)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import model_selection\n",
    "\n",
    "seed = 7\n",
    "kfold = model_selection.KFold(n_splits=10, random_state=seed)\n",
    "num_trees = 100\n",
    "\n",
    "clf = DecisionTreeClassifier()\n",
    "model = BaggingClassifier(base_estimator=clf, n_estimators=num_trees, random_state=seed)\n",
    "results = model_selection.cross_val_score(model, x_train, y_train, cv=kfold)\n",
    "\n",
    "print(results)\n",
    "print(\"max: \", results.max())\n",
    "print(\"min: \", results.min())\n",
    "print(\"mean: \", results.mean())\n",
    "\n",
    "# We get a robust estimate of model accuracy.\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (results.mean(), results.std() * 2)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Random Forest\n",
    "\n",
    "Random forest is an extension of bagged decision trees.\n",
    "\n",
    "Samples of the training dataset are taken with replacement, but the trees are constructed in a way that reduces the correlation between individual classifiers. Specifically, rather than greedily choosing the best split point in the construction of the tree, only a random subset of features are considered for each split.\n",
    "\n",
    "We can construct a Random Forest model for classification using the **RandomForestClassifier** (http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) class. A random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and use averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).\n",
    "\n",
    "The example below provides an example of Random Forest for classification with 100 trees and split points chosen from a random selection of 3 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.74157303  0.83146067  0.78651685  0.79775281  0.87640449  0.86516854\n",
      "  0.80898876  0.79775281  0.79775281  0.86516854]\n",
      "max:  0.876404494382\n",
      "min:  0.741573033708\n",
      "mean:  0.816853932584\n",
      "Accuracy: 0.82 (+/- 0.08)\n"
     ]
    }
   ],
   "source": [
    "# Random Forest Classification\n",
    "from sklearn import model_selection\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "seed = 7\n",
    "num_trees = 100\n",
    "max_features = 3\n",
    "\n",
    "kfold = model_selection.KFold(n_splits=10, random_state=seed)\n",
    "model = RandomForestClassifier(n_estimators=num_trees, max_features=max_features)\n",
    "results = model_selection.cross_val_score(model, x_train, y_train, cv=kfold)\n",
    "\n",
    "print(results)\n",
    "print(\"max: \", results.max())\n",
    "print(\"min: \", results.min())\n",
    "print(\"mean: \", results.mean())\n",
    "\n",
    "# We get a mean estimate of classification accuracy.\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (results.mean(), results.std() * 2)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Extra Trees\n",
    "\n",
    "Extra Trees are another modification of **bagging** where random trees are constructed from samples of the training dataset.\n",
    "\n",
    "You can construct an Extra Trees model for classification using the **ExtraTreesClassifier** (http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html) class. This class implements a meta estimator that fits a number of randomized decision trees (a.k.a. extra-trees) on various sub-samples of the dataset and use averaging to improve the predictive accuracy and control over-fitting.\n",
    "\n",
    "The example below provides a demonstration of extra trees with the number of trees set to 100 and splits chosen from 7 random features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.70786517  0.80898876  0.78651685  0.80898876  0.85393258  0.86516854\n",
      "  0.83146067  0.79775281  0.79775281  0.84269663]\n",
      "max:  0.865168539326\n",
      "min:  0.707865168539\n",
      "mean:  0.810112359551\n",
      "Accuracy: 0.81 (+/- 0.08)\n"
     ]
    }
   ],
   "source": [
    "# Extra Trees Classification\n",
    "import pandas\n",
    "from sklearn import model_selection\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "seed = 7\n",
    "num_trees = 100\n",
    "max_features = 7\n",
    "\n",
    "kfold = model_selection.KFold(n_splits=10, random_state=seed)\n",
    "model = ExtraTreesClassifier(n_estimators=num_trees, max_features=max_features)\n",
    "results = model_selection.cross_val_score(model, x_train, y_train, cv=kfold)\n",
    "\n",
    "print(results)\n",
    "print(\"max: \", results.max())\n",
    "print(\"min: \", results.min())\n",
    "print(\"mean: \", results.mean())\n",
    "\n",
    "# We get a mean estimate of classification accuracy.\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (results.mean(), results.std() * 2)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Boosting Algorithms\n",
    "\n",
    "Boosting ensemble algorithms creates a sequence of models that attempt to correct the mistakes of the models before them in the sequence.\n",
    "\n",
    "Once created, the models make predictions which may be weighted by their demonstrated accuracy and the results are combined to create a final output prediction.\n",
    "\n",
    "The two most common boosting ensemble machine learning algorithms are:\n",
    "\n",
    "+ AdaBoost\n",
    "+ Stochastic Gradient Boosting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost\n",
    "\n",
    "AdaBoost was perhaps the first successful boosting ensemble algorithm. It generally works by weighting instances in the dataset by how easy or difficult they are to classify, allowing the algorithm to pay or or less attention to them in the construction of subsequent models.\n",
    "\n",
    "You can construct an AdaBoost model for classification using the **AdaBoostClassifier** (http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html) class. An AdaBoost [1] classifier is a meta-estimator that begins by fitting a classifier on the original dataset and then fits additional copies of the classifier on the same dataset but where the weights of incorrectly classified instances are adjusted such that subsequent classifiers focus more on difficult cases. This class implements the algorithm known as AdaBoost-SAMME [2].\n",
    "\n",
    "The example below demonstrates the construction of 30 decision trees in sequence using the AdaBoost algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.79775281  0.80898876  0.80898876  0.82022472  0.83146067  0.80898876\n",
      "  0.83146067  0.80898876  0.86516854  0.84269663]\n",
      "max:  0.865168539326\n",
      "min:  0.797752808989\n",
      "mean:  0.822471910112\n",
      "Accuracy: 0.82 (+/- 0.04)\n"
     ]
    }
   ],
   "source": [
    "# AdaBoost Classification\n",
    "from sklearn import model_selection\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "seed = 7\n",
    "num_trees = 30\n",
    "\n",
    "kfold = model_selection.KFold(n_splits=10, random_state=seed)\n",
    "model = AdaBoostClassifier(n_estimators=num_trees, random_state=seed)\n",
    "results = model_selection.cross_val_score(model, x_train, y_train, cv=kfold)\n",
    "\n",
    "print(results)\n",
    "print(\"max: \", results.max())\n",
    "print(\"min: \", results.min())\n",
    "print(\"mean: \", results.mean())\n",
    "\n",
    "# We get a mean estimate of classification accuracy.\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (results.mean(), results.std() * 2)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Boosting\n",
    "\n",
    "Stochastic Gradient Boosting (also called Gradient Boosting Machines) are one of the most sophisticated ensemble techniques. It is also a technique that is proving to be perhaps of the the best techniques available for improving performance via ensembles.\n",
    "\n",
    "You can construct a Gradient Boosting model for classification using the **GradientBoostingClassifier** (http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html) class. Gradient Boosting for classification - GB builds an additive model in a forward stage-wise fashion; it allows for the optimization of arbitrary differentiable loss functions. In each stage **n\\_classes\\_** regression trees are fit on the negative gradient of the binomial or multinomial deviance loss function. Binary classification is a special case where only a single regression tree is induced.\n",
    "\n",
    "The example below demonstrates Stochastic Gradient Boosting for classification with 100 trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.78651685  0.82022472  0.79775281  0.84269663  0.87640449  0.84269663\n",
      "  0.83146067  0.78651685  0.88764045  0.82022472]\n",
      "max:  0.887640449438\n",
      "min:  0.786516853933\n",
      "mean:  0.829213483146\n",
      "Accuracy: 0.83 (+/- 0.07)\n"
     ]
    }
   ],
   "source": [
    "# Stochastic Gradient Boosting Classification\n",
    "from sklearn import model_selection\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "seed = 7\n",
    "num_trees = 100\n",
    "\n",
    "kfold = model_selection.KFold(n_splits=10, random_state=seed)\n",
    "model = GradientBoostingClassifier(n_estimators=num_trees, random_state=seed)\n",
    "results = model_selection.cross_val_score(model, x_train, y_train, cv=kfold)\n",
    "\n",
    "print(results)\n",
    "print(\"max: \", results.max())\n",
    "print(\"min: \", results.min())\n",
    "print(\"mean: \", results.mean())\n",
    "\n",
    "# We get a mean estimate of classification accuracy.\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (results.mean(), results.std() * 2)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voting Ensemble\n",
    "\n",
    "Voting is one of the simplest ways of combining the predictions from multiple machine learning algorithms.\n",
    "\n",
    "It works by first creating two or more standalone models from your training dataset. A Voting Classifier can then be used to wrap your models and average the predictions of the sub-models when asked to make predictions for new data.\n",
    "\n",
    "The predictions of the sub-models can be weighted, but specifying the weights for classifiers manually or even heuristically is difficult. More advanced methods can learn how to best weight the predictions from submodels, but this is called stacking (stacked aggregation) and is currently not provided in scikit-learn.\n",
    "\n",
    "You can create a voting ensemble model for classification using the **VotingClassifier** (http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html) class. Soft Voting/Majority Rule classifier for unfitted estimators. *New in version 0.17.*\n",
    "\n",
    "\n",
    "The code below provides an example of combining the predictions of logistic regression, classification and regression trees and support vector machines together for a classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.83146067  0.84269663  0.75280899  0.76404494  0.83146067  0.82022472\n",
      "  0.80898876  0.7752809   0.7752809   0.80898876]\n",
      "max:  0.842696629213\n",
      "min:  0.752808988764\n",
      "mean:  0.801123595506\n",
      "Accuracy: 0.80 (+/- 0.06)\n"
     ]
    }
   ],
   "source": [
    "# Voting Ensemble for Classification\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "seed = 7\n",
    "kfold = model_selection.KFold(n_splits=10, random_state=seed)\n",
    "\n",
    "# create the sub models\n",
    "estimators = []\n",
    "\n",
    "model1 = LogisticRegression()\n",
    "estimators.append(('logistic', model1))\n",
    "\n",
    "model2 = DecisionTreeClassifier()\n",
    "estimators.append(('cart', model2))\n",
    "\n",
    "model3 = SVC()\n",
    "estimators.append(('svm', model3))\n",
    "\n",
    "# create the ensemble model\n",
    "ensemble = VotingClassifier(estimators)\n",
    "results = model_selection.cross_val_score(ensemble, x_train, y_train, cv=kfold)\n",
    "\n",
    "print(results)\n",
    "print(\"max: \", results.max())\n",
    "print(\"min: \", results.min())\n",
    "print(\"mean: \", results.mean())\n",
    "\n",
    "# We get a mean estimate of classification accuracy.\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (results.mean(), results.std() * 2)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
